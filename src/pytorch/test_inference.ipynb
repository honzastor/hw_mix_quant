{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure inference time and evaluate the accuracies for different types of model states (FP32 | INT) and on different devices (CPU | GPU)\n",
    "###### Author: Jan Klhufek (iklhufek@fit.vut.cz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Tuple\n",
    "\n",
    "from data.data_loaders import ImagenetLoader, Imagenet100Loader, Cifar10Loader\n",
    "from utils.utils import *\n",
    "from models.mobilenet_v2 import mobilenetv2\n",
    "from models.mobilenet_v1 import mobilenetv1\n",
    "import eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_latency(model: nn.Module, device: str, input_size: Tuple[int, int, int, int] = (1, 3, 32, 32), num_samples: int = 100, num_warmups: int = 10, half_tensor: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Measures the average inference latency of a neural network model on a given device.\n",
    "    This function runs the model with a specified number of samples and warm-up iterations to measure the average inference time.\n",
    "\n",
    "    Code adapted and modified from: https://leimao.github.io/blog/PyTorch-Quantization-Aware-Training/\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to be evaluated.\n",
    "        device (str): The device (CPU or CUDA) on which to perform the inference.\n",
    "        input_size (Tuple[int, int, int, int]): The size of the input tensor. Defaults to (1, 3, 32, 32).\n",
    "        num_samples (int): The number of samples to run for measuring inference time. Defaults to 100.\n",
    "        num_warmups (int): The number of warm-up iterations to stabilize performance measurements. Defaults to 10.\n",
    "        half_tensor (bool): Whether to use half precision tensors for inference. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        float: The average time taken for inference per sample, in seconds.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    x = torch.rand(size=input_size).to(device)\n",
    "    x = x.half() if half_tensor else x\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_warmups):\n",
    "            _ = model(x)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        for _ in range(num_samples):\n",
    "            _ = model(x)\n",
    "            torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_time_ave = elapsed_time / num_samples\n",
    "\n",
    "    return elapsed_time_ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_equivalence(model_1: nn.Module, model_2: nn.Module, device: str, rtol: float = 1e-05, atol: float = 1e-08, num_tests: int = 100, input_size: Tuple[int, int, int, int] = (1,3,224,224)) -> bool:\n",
    "    \"\"\"\n",
    "    Checks the equivalence of two models (BEFORE and AFTER fusing, not conversion to int) by comparing their outputs on randomly generated inputs.\n",
    "    The equivalence is determined based on the relative tolerance (rtol) and absolute tolerance (atol) of the outputs.\n",
    "\n",
    "    Code adapted and modified from: https://leimao.github.io/blog/PyTorch-Quantization-Aware-Training/\n",
    "\n",
    "    Args:\n",
    "        model_1 (nn.Module): The first neural network model for comparison.\n",
    "        model_2 (nn.Module): The second neural network model for comparison.\n",
    "        device (str): The device (CPU or CUDA) on which to perform the tests.\n",
    "        rtol (float): The relative tolerance parameter for np.allclose. Defaults to 1e-05.\n",
    "        atol (float): The absolute tolerance parameter for np.allclose. Defaults to 1e-08.\n",
    "        num_tests (int): The number of random tests to run for comparison. Defaults to 100.\n",
    "        input_size (Tuple[int, int, int, int]): The size of the input tensor. Defaults to (1, 3, 224, 224).\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the models are equivalent within the specified tolerances, False otherwise.\n",
    "    \"\"\"\n",
    "    model_1.to(device)\n",
    "    model_2.to(device)\n",
    "    model_1.eval()\n",
    "    model_2.eval()\n",
    "\n",
    "    for _ in range(num_tests):\n",
    "        x = torch.rand(size=input_size).to(device)\n",
    "        y1 = model_1(x).detach().cpu().numpy()\n",
    "        y2 = model_2(x).detach().cpu().numpy()\n",
    "        if np.allclose(a=y1, b=y2, rtol=rtol, atol=atol, equal_nan=False) == False:\n",
    "            print(\"Model equivalence test sample failed: \")\n",
    "            print(y1)\n",
    "            print(y2)\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracies and measure the inference time of a model trained with floating-point precision\n",
    "fp32_data = \"checkpoints/no_QAT/mobilenetv2_cifar10_noqat_150epochs/model_best.pth.tar\"\n",
    "\n",
    "# Instantiate models (mobilenetv2 for example)\n",
    "cpu_mobilenet2 = mobilenetv2(num_classes=10, pretrained=True, checkpoint_path=fp32_data).to(\"cpu\")\n",
    "gpu_mobilenet2 = mobilenetv2(num_classes=10, pretrained=True, checkpoint_path=fp32_data).to(\"cuda\")\n",
    "fp16_gpu_mobilenet2 = mobilenetv2(num_classes=10, pretrained=True, checkpoint_path=fp32_data, half_tensor=True).to(\"cuda\")\n",
    "\n",
    "# Create data loaders\n",
    "cifar10_loader = Cifar10Loader()\n",
    "cpu_val_loader = cifar10_loader.load_validation_data(batch_size=512, num_workers=4, pin_memory=False)\n",
    "gpu_val_loader = cifar10_loader.load_validation_data(batch_size=512, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Evaluate inference accuracies\n",
    "criterion = nn.CrossEntropyLoss()  # Could also be None if we do not care about loss\n",
    "_, cpu_fp32_avg_top1, cpu_fp32_avg_top5 = eval.test(model=cpu_mobilenet2, val_loader=cpu_val_loader, criterion=criterion, device=\"cpu\")\n",
    "_, gpu_fp32_avg_top1, gpu_fp32_avg_top5 = eval.test(model=gpu_mobilenet2, val_loader=gpu_val_loader, criterion=criterion, device=\"cuda\")\n",
    "_, gpu_fp16_avg_top1, gpu_fp16_avg_top5 = eval.test(model=fp16_gpu_mobilenet2, val_loader=gpu_val_loader, criterion=criterion, device=\"cuda\", half_tensor=True)\n",
    "\n",
    "# Measure inference time over 100 random samples of batches of data\n",
    "fp32_cpu_inference_latency = measure_inference_latency(model=cpu_mobilenet2, device=\"cpu\", input_size=(512,3,32,32), num_samples=100)\n",
    "fp32_gpu_inference_latency = measure_inference_latency(model=gpu_mobilenet2, device=\"cuda\", input_size=(512,3,32,32), num_samples=100)\n",
    "fp16_gpu_inference_latency = measure_inference_latency(model=fp16_gpu_mobilenet2, device=\"cuda\", input_size=(512,3,32,32), num_samples=100, half_tensor=True)\n",
    "\n",
    "# CPU FP32\n",
    "print(f\"\\nFP32 CPU Inference TOP1 Acc: {cpu_fp32_avg_top1}\")\n",
    "print(f\"FP32 CPU Inference TOP5 Acc: {cpu_fp32_avg_top5}\")\n",
    "print(f\"FP32 CPU Inference Latency: {(fp32_cpu_inference_latency * 1000):.2f} ms / sample \\n\")\n",
    "\n",
    "# GPU FP32\n",
    "print(f\"FP32 GPU Inference TOP1 Acc: {gpu_fp32_avg_top1}\")\n",
    "print(f\"FP32 GPU Inference TOP5 Acc: {gpu_fp32_avg_top5}\")\n",
    "print(f\"FP32 GPU Inference Latency: {(fp32_gpu_inference_latency * 1000):.2f} ms / sample \\n\")\n",
    "\n",
    "# GPU FP16\n",
    "print(f\"FP16 GPU Inference TOP1 Acc: {gpu_fp16_avg_top1}\")\n",
    "print(f\"FP16 GPU Inference TOP5 Acc: {gpu_fp16_avg_top5}\")\n",
    "print(f\"FP16 GPU Inference Latency: {(fp16_gpu_inference_latency * 1000):.2f} ms / sample\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check equivalence of model before and after layer fusion (which precedes QAT)\n",
      "Creating custom MobileNetV1 model and initiliazing weights..\n",
      "Models are functionaly equivalent!\n",
      "Creating custom MobileNetV1 model and initiliazing weights..\n",
      "Loading QAT state model\n",
      "\n",
      "Creating custom MobileNetV1 model and initiliazing weights..\n",
      "Loading quantized model\n",
      "\n",
      "Original model Size: 13.29 MB\n",
      "Quantized model Size: 3.27 MB\n",
      "\n",
      "Evaluate floating-point model on GPU before conversion to integer.\n",
      "Creating validation DataLoader..\n",
      "Test: [ 0/20]\tBatch time 0.475 (0.475)\tLoss 1.116 (1.116)\tTop1 acc 82.812 (82.812)\tTop5 acc 98.828 (98.828)\n",
      "Test: [ 1/20]\tBatch time 0.056 (0.266)\tLoss 0.718 (0.917)\tTop1 acc 85.352 (84.082)\tTop5 acc 99.023 (98.926)\n",
      "Test: [ 2/20]\tBatch time 0.057 (0.196)\tLoss 0.391 (0.742)\tTop1 acc 92.773 (86.979)\tTop5 acc 99.609 (99.154)\n",
      "Test: [ 3/20]\tBatch time 0.056 (0.161)\tLoss 0.736 (0.740)\tTop1 acc 88.867 (87.451)\tTop5 acc 99.414 (99.219)\n",
      "Test: [ 4/20]\tBatch time 0.092 (0.147)\tLoss 1.587 (0.910)\tTop1 acc 75.195 (85.000)\tTop5 acc 98.242 (99.023)\n",
      "Test: [ 5/20]\tBatch time 0.057 (0.132)\tLoss 1.601 (1.025)\tTop1 acc 74.219 (83.203)\tTop5 acc 98.438 (98.926)\n",
      "Test: [ 6/20]\tBatch time 0.055 (0.121)\tLoss 2.142 (1.184)\tTop1 acc 65.234 (80.636)\tTop5 acc 98.047 (98.800)\n",
      "Test: [ 7/20]\tBatch time 0.060 (0.113)\tLoss 2.119 (1.301)\tTop1 acc 68.945 (79.175)\tTop5 acc 99.219 (98.853)\n",
      "Test: [ 8/20]\tBatch time 0.111 (0.113)\tLoss 1.234 (1.294)\tTop1 acc 80.664 (79.340)\tTop5 acc 98.633 (98.828)\n",
      "Test: [ 9/20]\tBatch time 0.077 (0.110)\tLoss 1.378 (1.302)\tTop1 acc 78.711 (79.277)\tTop5 acc 98.633 (98.809)\n",
      "Test: [10/20]\tBatch time 0.083 (0.107)\tLoss 1.457 (1.316)\tTop1 acc 74.805 (78.871)\tTop5 acc 97.266 (98.668)\n",
      "Test: [11/20]\tBatch time 0.077 (0.105)\tLoss 1.551 (1.336)\tTop1 acc 74.805 (78.532)\tTop5 acc 99.023 (98.698)\n",
      "Test: [12/20]\tBatch time 0.086 (0.103)\tLoss 0.673 (1.285)\tTop1 acc 87.695 (79.237)\tTop5 acc 99.219 (98.738)\n",
      "Test: [13/20]\tBatch time 0.058 (0.100)\tLoss 0.586 (1.235)\tTop1 acc 89.453 (79.967)\tTop5 acc 100.000 (98.828)\n",
      "Test: [14/20]\tBatch time 0.048 (0.097)\tLoss 0.933 (1.215)\tTop1 acc 85.156 (80.312)\tTop5 acc 99.023 (98.841)\n",
      "Test: [15/20]\tBatch time 0.049 (0.094)\tLoss 0.786 (1.188)\tTop1 acc 86.328 (80.688)\tTop5 acc 98.438 (98.816)\n",
      "Test: [16/20]\tBatch time 0.056 (0.091)\tLoss 0.741 (1.162)\tTop1 acc 89.648 (81.216)\tTop5 acc 99.219 (98.840)\n",
      "Test: [17/20]\tBatch time 0.041 (0.089)\tLoss 0.719 (1.137)\tTop1 acc 87.500 (81.565)\tTop5 acc 99.414 (98.872)\n",
      "Test: [18/20]\tBatch time 0.041 (0.086)\tLoss 0.542 (1.106)\tTop1 acc 90.430 (82.031)\tTop5 acc 99.805 (98.921)\n",
      "Test: [19/20]\tBatch time 0.035 (0.083)\tLoss 0.534 (1.090)\tTop1 acc 90.074 (82.250)\tTop5 acc 100.000 (98.950)\n",
      " * Batch time 0.083 Loss 1.090 Top1 acc 82.250 Top5 acc 98.950\n",
      "FP32 GPU Inference TOP1 Acc: 82.24999986572266\n",
      "FP32 GPU Inference TOP5 Acc: 98.95\n",
      "FP32 GPU Inference Latency: 38.02 ms / sample \n",
      "\n",
      "Creating validation DataLoader..\n",
      "Evaluate integer (loaded) model.\n",
      "Test: [ 0/20]\tBatch time 0.575 (0.575)\tLoss 1.907 (1.907)\tTop1 acc 71.289 (71.289)\tTop5 acc 98.633 (98.633)\n",
      "Test: [ 1/20]\tBatch time 0.161 (0.368)\tLoss 1.613 (1.760)\tTop1 acc 73.242 (72.266)\tTop5 acc 99.023 (98.828)\n",
      "Test: [ 2/20]\tBatch time 0.158 (0.298)\tLoss 0.470 (1.330)\tTop1 acc 91.992 (78.841)\tTop5 acc 99.805 (99.154)\n",
      "Test: [ 3/20]\tBatch time 0.120 (0.254)\tLoss 0.693 (1.171)\tTop1 acc 88.281 (81.201)\tTop5 acc 99.219 (99.170)\n",
      "Test: [ 4/20]\tBatch time 0.112 (0.225)\tLoss 2.171 (1.371)\tTop1 acc 66.406 (78.242)\tTop5 acc 96.484 (98.633)\n",
      "Test: [ 5/20]\tBatch time 0.098 (0.204)\tLoss 2.334 (1.531)\tTop1 acc 68.359 (76.595)\tTop5 acc 96.875 (98.340)\n",
      "Test: [ 6/20]\tBatch time 0.153 (0.197)\tLoss 2.596 (1.683)\tTop1 acc 58.789 (74.051)\tTop5 acc 96.484 (98.075)\n",
      "Test: [ 7/20]\tBatch time 0.149 (0.191)\tLoss 2.391 (1.772)\tTop1 acc 63.867 (72.778)\tTop5 acc 96.875 (97.925)\n",
      "Test: [ 8/20]\tBatch time 0.103 (0.181)\tLoss 1.261 (1.715)\tTop1 acc 79.297 (73.503)\tTop5 acc 98.438 (97.982)\n",
      "Test: [ 9/20]\tBatch time 0.106 (0.174)\tLoss 1.838 (1.727)\tTop1 acc 75.195 (73.672)\tTop5 acc 96.484 (97.832)\n",
      "Test: [10/20]\tBatch time 0.104 (0.167)\tLoss 2.420 (1.790)\tTop1 acc 64.844 (72.869)\tTop5 acc 95.312 (97.603)\n",
      "Test: [11/20]\tBatch time 0.098 (0.162)\tLoss 2.301 (1.833)\tTop1 acc 68.164 (72.477)\tTop5 acc 96.094 (97.477)\n",
      "Test: [12/20]\tBatch time 0.088 (0.156)\tLoss 0.629 (1.740)\tTop1 acc 87.891 (73.663)\tTop5 acc 99.023 (97.596)\n",
      "Test: [13/20]\tBatch time 0.082 (0.151)\tLoss 0.710 (1.667)\tTop1 acc 86.719 (74.595)\tTop5 acc 99.219 (97.712)\n",
      "Test: [14/20]\tBatch time 0.079 (0.146)\tLoss 1.282 (1.641)\tTop1 acc 79.297 (74.909)\tTop5 acc 97.266 (97.682)\n",
      "Test: [15/20]\tBatch time 0.081 (0.142)\tLoss 1.142 (1.610)\tTop1 acc 81.641 (75.330)\tTop5 acc 97.266 (97.656)\n",
      "Test: [16/20]\tBatch time 0.084 (0.138)\tLoss 0.725 (1.558)\tTop1 acc 86.719 (76.000)\tTop5 acc 99.023 (97.737)\n",
      "Test: [17/20]\tBatch time 0.081 (0.135)\tLoss 0.901 (1.521)\tTop1 acc 83.008 (76.389)\tTop5 acc 98.438 (97.776)\n",
      "Test: [18/20]\tBatch time 0.081 (0.132)\tLoss 0.708 (1.478)\tTop1 acc 85.352 (76.861)\tTop5 acc 99.805 (97.882)\n",
      "Test: [19/20]\tBatch time 0.051 (0.128)\tLoss 0.804 (1.460)\tTop1 acc 85.294 (77.090)\tTop5 acc 99.632 (97.930)\n",
      " * Batch time 0.128 Loss 1.460 Top1 acc 77.090 Top5 acc 97.930\n",
      "\n",
      "Loaded Model INT8 CPU Inference TOP1 Acc: 77.08999987792969\n",
      "Loaded Model INT8 CPU Inference TOP5 Acc: 97.93000004882812\n",
      "Loaded Model INT8 CPU Inference Latency: 68.96 ms / sample \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracies and measure the inference time of a model trained using QAT and then converted into integer precision\n",
    "print(\"Check equivalence of model before and after layer fusion (which precedes QAT)\")\n",
    "test_model = mobilenetv1(num_classes=10).to(\"cpu\")\n",
    "fused_test_model = copy.deepcopy(test_model)\n",
    "fused_test_model._qat = True\n",
    "fused_test_model.quant = torch.ao.quantization.QuantStub()\n",
    "fused_test_model.dequant = torch.ao.quantization.DeQuantStub()\n",
    "fused_test_model._quant_config = {}\n",
    "fused_test_model._set_qat_config()\n",
    "fused_test_model.eval()\n",
    "fused_test_model.fuse_model()\n",
    "fused_test_model.train()\n",
    "torch.quantization.prepare_qat(fused_test_model, inplace=True)\n",
    "\n",
    "# Check that models are equivalent\n",
    "assert model_equivalence(model_1=test_model, model_2=fused_test_model, device=\"cpu\", rtol=1e-03, atol=1e-06, num_tests=100, input_size=(64,3,32,32)), \"Quantized model deviates from the original model too much!\"\n",
    "print(\"Models are functionaly equivalent!\")\n",
    "\n",
    "# Test accuracies and inference times\n",
    "mn1_qat_pre_conversion_sym_chkpt = \"checkpoints/QAT_sym/mobilenetv1_cifar10_qat_150epochs/model_best.pth.tar\"\n",
    "mn1_qat_jit_converted_sym_chkpt = \"checkpoints/QAT_sym/mobilenetv1_cifar10_qat_150epochs/jit_model_after_qat.pth.tar\"\n",
    "mn1_qat_converted_sym_chkpt = \"checkpoints/QAT_sym/mobilenetv1_cifar10_qat_150epochs/model_after_qat.pth.tar\"\n",
    "orig_mobilenet1 = mobilenetv1(num_classes=10, pretrained=True, checkpoint_path=mn1_qat_pre_conversion_sym_chkpt, qat=True, symmetric_quant=True, act_function=nn.ReLU).to(\"cpu\")\n",
    "quant_mobilenet1 = mobilenetv1(num_classes=10, pretrained=True, checkpoint_path=mn1_qat_converted_sym_chkpt, qat=True, symmetric_quant=True, load_quantized=True, act_function=nn.ReLU).to(\"cpu\")\n",
    "quant_jit_mobilenet1 = torch.jit.load(mn1_qat_jit_converted_sym_chkpt, map_location=\"cpu\")\n",
    "\n",
    "# Print original model size and size after quantization\n",
    "orig_model_size = get_model_size(orig_mobilenet1)\n",
    "quant_model_size = get_model_size(quant_mobilenet1)\n",
    "print(f\"Original model Size: {orig_model_size:.2f} MB\")\n",
    "print(f\"Quantized model Size: {quant_model_size:.2f} MB\\n\")\n",
    "\n",
    "# FP32 PRECISION (USING GPU)\n",
    "print(\"Evaluate floating-point model on GPU before conversion to integer.\") # Could also be tested on CPU, but for larger models/datasets, that is not possible in most cases\n",
    "cifar10_loader = Cifar10Loader()\n",
    "cpu_val_loader = cifar10_loader.load_validation_data(batch_size=512, num_workers=4, pin_memory=False)\n",
    "gpu_val_loader = cifar10_loader.load_validation_data(batch_size=512, num_workers=4, pin_memory=True)\n",
    "orig_mobilenet1.to(\"cuda\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Could also be None if we do not care about loss\n",
    "_, gpu_fp32_avg_top1, gpu_fp32_avg_top5 = eval.test(model=orig_mobilenet1, val_loader=gpu_val_loader, criterion=criterion, device=\"cuda\")\n",
    "# Measure inference time over 100 random samples of batches of data\n",
    "fp32_gpu_inference_latency = measure_inference_latency(model=orig_mobilenet1, device=\"cuda\", input_size=(512,3,32,32), num_samples=100)\n",
    "\n",
    "# orig_mobilenet1.to(\"cpu\")\n",
    "# _, cpu_fp32_avg_top1, cpu_fp32_avg_top5 = eval.test(model=orig_mobilenet1, val_loader=cpu_val_loader, criterion=criterion, device=\"cpu\")\n",
    "# Measure inference time over 100 random samples of batches of data\n",
    "# fp32_cpu_inference_latency = measure_inference_latency(model=orig_mobilenet1, device=\"cpu\", input_size=(512,3,32,32), num_samples=100)\n",
    "\n",
    "# GPU FP32\n",
    "print(f\"FP32 GPU Inference TOP1 Acc: {gpu_fp32_avg_top1}\")\n",
    "print(f\"FP32 GPU Inference TOP5 Acc: {gpu_fp32_avg_top5}\")\n",
    "print(f\"FP32 GPU Inference Latency: {(fp32_gpu_inference_latency * 1000):.2f} ms / sample \\n\")\n",
    "# CPU FP32 – optional (but good for comparison with quantized inference time)\n",
    "# print(f\"FP32 CPU Inference TOP1 Acc: {cpu_fp32_avg_top1}\")\n",
    "# print(f\"FP32 CPU Inference TOP5 Acc: {cpu_fp32_avg_top5}\")\n",
    "# print(f\"FP32 CPU Inference Latency: {(fp32_cpu_inference_latency * 1000):.2f} ms / sample \\n\")\n",
    "\n",
    "# CONVERT MODEL INPLACE (if desired)\n",
    "#print(\"Converting the FP32 model (its state dict after QAT) into INT\")\n",
    "#orig_mobilenet1.to(\"cpu\")  # NOTE: Quantization operations in PyTorch are optimized for CPU backend inference (i.e. utilization of vectorization, etc.).\n",
    "#orig_mobilenet1.eval()\n",
    "#torch.ao.quantization.convert(orig_mobilenet1, inplace=True)\n",
    "\n",
    "# Measure int inference\n",
    "print(\"Evaluate integer (loaded) model.\")\n",
    "_, cpu_load_int8_avg_top1, cpu_load_int8_avg_top5 = eval.test(model=quant_mobilenet1, val_loader=cpu_val_loader, criterion=criterion, device=\"cpu\")\n",
    "\n",
    "# Measure inference time over 100 random samples of batches of data for the converted model\n",
    "int8_cpu_load_inference_latency = measure_inference_latency(model=quant_jit_mobilenet1, device=\"cpu\", input_size=(512,3,32,32), num_samples=100)\n",
    "\n",
    "# CPU After QAT INT8\n",
    "print(f\"\\nLoaded Model INT8 CPU Inference TOP1 Acc: {cpu_load_int8_avg_top1}\")\n",
    "print(f\"Loaded Model INT8 CPU Inference TOP5 Acc: {cpu_load_int8_avg_top5}\")\n",
    "print(f\"Loaded Model INT8 CPU Inference Latency: {(int8_cpu_load_inference_latency * 1000):.2f} ms / sample \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch kernel",
   "language": "python",
   "name": "haq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
