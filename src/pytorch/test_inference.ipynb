{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure inference time and evaluate the accuracies for different types of model states (FP32 | INT) and on different devices (CPU | GPU)\n",
    "###### Author: Jan Klhufek (iklhufek@fit.vut.cz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Tuple\n",
    "\n",
    "from data.data_loaders import ImagenetLoader, Imagenet100Loader, Cifar10Loader\n",
    "from utils.utils import *\n",
    "from models.mobilenet_v2 import mobilenetv2\n",
    "from models.mobilenet_v1 import mobilenetv1\n",
    "import eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_latency(model: nn.Module, device: str, input_size: Tuple[int, int, int, int] = (1, 3, 32, 32), num_samples: int = 100, num_warmups: int = 10, half_tensor: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Measures the average inference latency of a neural network model on a given device.\n",
    "    This function runs the model with a specified number of samples and warm-up iterations to measure the average inference time.\n",
    "\n",
    "    Code adapted and modified from: https://leimao.github.io/blog/PyTorch-Quantization-Aware-Training/\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to be evaluated.\n",
    "        device (str): The device (CPU or CUDA) on which to perform the inference.\n",
    "        input_size (Tuple[int, int, int, int]): The size of the input tensor. Defaults to (1, 3, 32, 32).\n",
    "        num_samples (int): The number of samples to run for measuring inference time. Defaults to 100.\n",
    "        num_warmups (int): The number of warm-up iterations to stabilize performance measurements. Defaults to 10.\n",
    "        half_tensor (bool): Whether to use half precision tensors for inference. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        float: The average time taken for inference per sample, in seconds.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    x = torch.rand(size=input_size).to(device)\n",
    "    x = x.half() if half_tensor else x\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_warmups):\n",
    "            _ = model(x)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        for _ in range(num_samples):\n",
    "            _ = model(x)\n",
    "            torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_time_ave = elapsed_time / num_samples\n",
    "\n",
    "    return elapsed_time_ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_equivalence(model_1: nn.Module, model_2: nn.Module, device: str, rtol: float = 1e-05, atol: float = 1e-08, num_tests: int = 100, input_size: Tuple[int, int, int, int] = (1,3,224,224)) -> bool:\n",
    "    \"\"\"\n",
    "    Checks the equivalence of two models (BEFORE and AFTER fusing, not conversion to int) by comparing their outputs on randomly generated inputs.\n",
    "    The equivalence is determined based on the relative tolerance (rtol) and absolute tolerance (atol) of the outputs.\n",
    "\n",
    "    Code adapted and modified from: https://leimao.github.io/blog/PyTorch-Quantization-Aware-Training/\n",
    "\n",
    "    Args:\n",
    "        model_1 (nn.Module): The first neural network model for comparison.\n",
    "        model_2 (nn.Module): The second neural network model for comparison.\n",
    "        device (str): The device (CPU or CUDA) on which to perform the tests.\n",
    "        rtol (float): The relative tolerance parameter for np.allclose. Defaults to 1e-05.\n",
    "        atol (float): The absolute tolerance parameter for np.allclose. Defaults to 1e-08.\n",
    "        num_tests (int): The number of random tests to run for comparison. Defaults to 100.\n",
    "        input_size (Tuple[int, int, int, int]): The size of the input tensor. Defaults to (1, 3, 224, 224).\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the models are equivalent within the specified tolerances, False otherwise.\n",
    "    \"\"\"\n",
    "    model_1.to(device)\n",
    "    model_2.to(device)\n",
    "    model_1.eval()\n",
    "    model_2.eval()\n",
    "\n",
    "    for _ in range(num_tests):\n",
    "        x = torch.rand(size=input_size).to(device)\n",
    "        y1 = model_1(x).detach().cpu().numpy()\n",
    "        y2 = model_2(x).detach().cpu().numpy()\n",
    "        if np.allclose(a=y1, b=y2, rtol=rtol, atol=atol, equal_nan=False) == False:\n",
    "            print(\"Model equivalence test sample failed: \")\n",
    "            print(y1)\n",
    "            print(y2)\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracies and measure the inference time of a model trained with floating-point precision\n",
    "fp32_data = \"checkpoints/no_QAT/mobilenetv2_cifar10_noqat_150epochs/model_best.pth.tar\"\n",
    "\n",
    "# Instantiate models (mobilenetv2 for example)\n",
    "cpu_mobilenet2 = mobilenetv2(num_classes=10, pretrained=True, checkpoint_path=fp32_data).to(\"cpu\")\n",
    "gpu_mobilenet2 = mobilenetv2(num_classes=10, pretrained=True, checkpoint_path=fp32_data).to(\"cuda\")\n",
    "fp16_gpu_mobilenet2 = mobilenetv2(num_classes=10, pretrained=True, checkpoint_path=fp32_data, half_tensor=True).to(\"cuda\")\n",
    "\n",
    "# Create data loaders\n",
    "cifar10_loader = Cifar10Loader()\n",
    "cpu_val_loader = cifar10_loader.load_validation_data(batch_size=512, num_workers=4, pin_memory=False)\n",
    "gpu_val_loader = cifar10_loader.load_validation_data(batch_size=512, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Evaluate inference accuracies\n",
    "criterion = nn.CrossEntropyLoss()  # Could also be None if we do not care about loss\n",
    "_, cpu_fp32_avg_top1, cpu_fp32_avg_top5 = eval.test(model=cpu_mobilenet2, val_loader=cpu_val_loader, criterion=criterion, device=\"cpu\")\n",
    "_, gpu_fp32_avg_top1, gpu_fp32_avg_top5 = eval.test(model=gpu_mobilenet2, val_loader=gpu_val_loader, criterion=criterion, device=\"cuda\")\n",
    "_, gpu_fp16_avg_top1, gpu_fp16_avg_top5 = eval.test(model=fp16_gpu_mobilenet2, val_loader=gpu_val_loader, criterion=criterion, device=\"cuda\", half_tensor=True)\n",
    "\n",
    "# Measure inference time over 100 random samples of batches of data\n",
    "fp32_cpu_inference_latency = measure_inference_latency(model=cpu_mobilenet2, device=\"cpu\", input_size=(512,3,32,32), num_samples=100)\n",
    "fp32_gpu_inference_latency = measure_inference_latency(model=gpu_mobilenet2, device=\"cuda\", input_size=(512,3,32,32), num_samples=100)\n",
    "fp16_gpu_inference_latency = measure_inference_latency(model=fp16_gpu_mobilenet2, device=\"cuda\", input_size=(512,3,32,32), num_samples=100, half_tensor=True)\n",
    "\n",
    "# CPU FP32\n",
    "print(f\"\\nFP32 CPU Inference TOP1 Acc: {cpu_fp32_avg_top1}\")\n",
    "print(f\"FP32 CPU Inference TOP5 Acc: {cpu_fp32_avg_top5}\")\n",
    "print(f\"FP32 CPU Inference Latency: {(fp32_cpu_inference_latency * 1000):.2f} ms / sample \\n\")\n",
    "\n",
    "# GPU FP32\n",
    "print(f\"FP32 GPU Inference TOP1 Acc: {gpu_fp32_avg_top1}\")\n",
    "print(f\"FP32 GPU Inference TOP5 Acc: {gpu_fp32_avg_top5}\")\n",
    "print(f\"FP32 GPU Inference Latency: {(fp32_gpu_inference_latency * 1000):.2f} ms / sample \\n\")\n",
    "\n",
    "# GPU FP16\n",
    "print(f\"FP16 GPU Inference TOP1 Acc: {gpu_fp16_avg_top1}\")\n",
    "print(f\"FP16 GPU Inference TOP5 Acc: {gpu_fp16_avg_top5}\")\n",
    "print(f\"FP16 GPU Inference Latency: {(fp16_gpu_inference_latency * 1000):.2f} ms / sample\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracies and measure the inference time of a model trained using QAT and then converted into integer precision\n",
    "print(\"Check equivalence of model before and after layer fusion (which precedes QAT)\")\n",
    "test_model = mobilenetv1(num_classes=10).to(\"cpu\")\n",
    "fused_test_model = copy.deepcopy(test_model)\n",
    "fused_test_model._qat = True\n",
    "fused_test_model.quant = torch.ao.quantization.QuantStub()\n",
    "fused_test_model.dequant = torch.ao.quantization.DeQuantStub()\n",
    "fused_test_model._quant_config = {}\n",
    "fused_test_model._set_qat_config()\n",
    "fused_test_model.eval()\n",
    "fused_test_model.fuse_model()\n",
    "fused_test_model.train()\n",
    "torch.quantization.prepare_qat(fused_test_model, inplace=True)\n",
    "\n",
    "# Check that models are equivalent\n",
    "assert model_equivalence(model_1=test_model, model_2=fused_test_model, device=\"cpu\", rtol=1e-03, atol=1e-06, num_tests=100, input_size=(64,3,32,32)), \"Quantized model deviates from the original model too much!\"\n",
    "print(\"Models are functionaly equivalent!\")\n",
    "\n",
    "# Test accuracies and inference times\n",
    "mn1_qat_pre_conversion_sym_chkpt = \"checkpoints/QAT_sym/mobilenetv1_cifar10_qat_150epochs/model_best.pth.tar\"\n",
    "mn1_qat_jit_converted_sym_chkpt = \"checkpoints/QAT_sym/mobilenetv1_cifar10_qat_150epochs/jit_model_after_qat.pth.tar\"\n",
    "mn1_qat_converted_sym_chkpt = \"checkpoints/QAT_sym/mobilenetv1_cifar10_qat_150epochs/model_after_qat.pth.tar\"\n",
    "orig_mobilenet1 = mobilenetv1(num_classes=10, pretrained=True, checkpoint_path=mn1_qat_pre_conversion_sym_chkpt, qat=True, symmetric_quant=True, act_function=nn.ReLU).to(\"cpu\")\n",
    "quant_mobilenet1 = mobilenetv1(num_classes=10, pretrained=True, checkpoint_path=mn1_qat_converted_sym_chkpt, qat=True, symmetric_quant=True, load_quantized=True, act_function=nn.ReLU).to(\"cpu\")\n",
    "quant_jit_mobilenet1 = torch.jit.load(mn1_qat_jit_converted_sym_chkpt, map_location=\"cpu\")\n",
    "\n",
    "# Print original model size and size after quantization\n",
    "orig_model_size = get_model_size(orig_mobilenet1)\n",
    "quant_model_size = get_model_size(quant_mobilenet1)\n",
    "print(f\"Original model Size: {orig_model_size:.2f} MB\")\n",
    "print(f\"Quantized model Size: {quant_model_size:.2f} MB\\n\")\n",
    "\n",
    "# FP32 PRECISION (USING GPU)\n",
    "print(\"Evaluate floating-point model on GPU before conversion to integer.\") # Could also be tested on CPU, but for larger models/datasets, that is not possible in most cases\n",
    "cifar10_loader = Cifar10Loader()\n",
    "cpu_val_loader = cifar10_loader.load_validation_data(batch_size=512, num_workers=4, pin_memory=False)\n",
    "gpu_val_loader = cifar10_loader.load_validation_data(batch_size=512, num_workers=4, pin_memory=True)\n",
    "orig_mobilenet1.to(\"cuda\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Could also be None if we do not care about loss\n",
    "_, gpu_fp32_avg_top1, gpu_fp32_avg_top5 = eval.test(model=orig_mobilenet1, val_loader=gpu_val_loader, criterion=criterion, device=\"cuda\")\n",
    "# Measure inference time over 100 random samples of batches of data\n",
    "fp32_gpu_inference_latency = measure_inference_latency(model=orig_mobilenet1, device=\"cuda\", input_size=(512,3,32,32), num_samples=100)\n",
    "\n",
    "# orig_mobilenet1.to(\"cpu\")\n",
    "# _, cpu_fp32_avg_top1, cpu_fp32_avg_top5 = eval.test(model=orig_mobilenet1, val_loader=cpu_val_loader, criterion=criterion, device=\"cpu\")\n",
    "# Measure inference time over 100 random samples of batches of data\n",
    "# fp32_cpu_inference_latency = measure_inference_latency(model=orig_mobilenet1, device=\"cpu\", input_size=(512,3,32,32), num_samples=100)\n",
    "\n",
    "# GPU FP32\n",
    "print(f\"FP32 GPU Inference TOP1 Acc: {gpu_fp32_avg_top1}\")\n",
    "print(f\"FP32 GPU Inference TOP5 Acc: {gpu_fp32_avg_top5}\")\n",
    "print(f\"FP32 GPU Inference Latency: {(fp32_gpu_inference_latency * 1000):.2f} ms / sample \\n\")\n",
    "# CPU FP32 â€“ optional (but good for comparison with quantized inference time)\n",
    "# print(f\"FP32 CPU Inference TOP1 Acc: {cpu_fp32_avg_top1}\")\n",
    "# print(f\"FP32 CPU Inference TOP5 Acc: {cpu_fp32_avg_top5}\")\n",
    "# print(f\"FP32 CPU Inference Latency: {(fp32_cpu_inference_latency * 1000):.2f} ms / sample \\n\")\n",
    "\n",
    "# CONVERT MODEL INPLACE (if desired)\n",
    "# print(\"Converting the FP32 model (its state dict after QAT) into INT\")\n",
    "# orig_mobilenet1.to(\"cpu\")  # NOTE: Quantization operations in PyTorch are optimized for CPU backend inference (i.e. utilization of vectorization, etc.).\n",
    "# orig_mobilenet1.eval()\n",
    "# torch.ao.quantization.convert(orig_mobilenet1, inplace=True)\n",
    "\n",
    "# Measure int inference\n",
    "print(\"Evaluate integer (loaded) model.\")\n",
    "_, cpu_load_int8_avg_top1, cpu_load_int8_avg_top5 = eval.test(model=quant_mobilenet1, val_loader=cpu_val_loader, criterion=criterion, device=\"cpu\")\n",
    "\n",
    "# Measure inference time over 100 random samples of batches of data for the converted model\n",
    "int8_cpu_load_inference_latency = measure_inference_latency(model=quant_jit_mobilenet1, device=\"cpu\", input_size=(512,3,32,32), num_samples=100)\n",
    "\n",
    "# CPU After QAT INT8\n",
    "print(f\"\\nLoaded Model INT8 CPU Inference TOP1 Acc: {cpu_load_int8_avg_top1}\")\n",
    "print(f\"Loaded Model INT8 CPU Inference TOP5 Acc: {cpu_load_int8_avg_top5}\")\n",
    "print(f\"Loaded Model INT8 CPU Inference Latency: {(int8_cpu_load_inference_latency * 1000):.2f} ms / sample \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch kernel",
   "language": "python",
   "name": "haq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
